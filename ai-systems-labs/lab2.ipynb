{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Л2: Использование Inference API для работы с моделями ИИ\n",
    "\n",
    "**Inference API** - это сервис, который обеспечивает быстрый доступ к многочисленным предобученным моделями ИИ, размещенным на инфраструктуре HuggingFace. Используя его нет необходимости в загрузке модели и запуску модели локально. Таким образом можно создавать прототипы приложений для решения различных задач не переживая о вычислительных мощностях своего компьютера. \n",
    "\n",
    "**Inference API** предоставляет бесплатный и мгновенный доступ к популярным и эффективным моделями для решения следующего спектра задача:\n",
    "* Генерация текста: включает большие языковые модели и подсказки для вызова инструментов, генерируйте и экспериментируйте с высококачественными ответами.\n",
    "* Генерация изображений: легко создавайте персонализированные изображения.\n",
    "* Работа с документами.\n",
    "* Классические задачи ИИ: готовые к использованию модели для классификации текста, изображений, распознавания речи и многого другого.\n",
    "\n",
    "\n",
    "**В этой работе мы познакомимся с этим инструментом и в качестве примера создадим собственного ассистента.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Подготовка среды выполнения\n",
    "На данной этапе вам наобходимо подготовить виртуальное окружение и установить все необходимые библиотеки.\n",
    "\n",
    "1. Создать и активировать (или только активировать, если ранне создавали) виртуальной окружение `python`.\n",
    "\n",
    "В терминале вводим следующие команды команды:\n",
    "\n",
    "*Создаем виртуальное окружение с помощью `python-venv`*\n",
    "```\n",
    "python -m venv env\n",
    "```\n",
    "*Активируем виртуальное окружение*\n",
    "для CMD\n",
    "```\n",
    "env\\Scripts\\activate\n",
    "```\n",
    "для PowerShell\n",
    "```\n",
    "env\\Scripts\\Activate.ps1\n",
    "```\n",
    "**Примечание.** `env` - это название вашего виртуального окружения, назвать его можете как угодно.\n",
    "\n",
    "После этого можем выбрать наш локальный интерпрететор pyhton, нажав на кнопку выше \"Select kernel\".\n",
    "\n",
    "2. Устанавливаем все необходимые библиотеки\n",
    "\n",
    "**Примечание.** Библиотеки установятся внутрь вашего виртуального окружения.\n",
    "\n",
    "Нам понадобятся библиотеки Diffusers, Transformers, Accelerate.\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "Также для работы вышеперечисленных бибилиотек потребуется PyTorch:\n",
    "```\n",
    "pip install torch\n",
    "```\n",
    "Библиотека Gradio для создания web-приложения.\n",
    "```\n",
    "pip install gradio\n",
    "```\n",
    "Также для работы с изображение нам потребуется библиотека PIL (Python Image Library) `Pillow`\n",
    "```\n",
    "pip install Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Начало рабты. Создание Inference Client\n",
    "\n",
    "Для бессерверного обращение к модели (то есть без запуска ее на своем каком-то сервере или локально) необходимо отправить запрос, используя Inference API. В результате этого запроса мы получим ответ - Inference (то есть вывод модели). Запрос можно формировать разынми способами, но в этой работе предлагается использовать Python библиотеку `huggingface_hub`. Выполните установку\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта библиотека предоставляет модуль `InferenceClient`, с помощью которого создадим клиент, который будет формировать запрос на сервера Hugging Face. Импортируем этот модуль. Также импортируете Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть одно НО! Для использования Serverless Inference API необходимо иметь **токен доступа**. Чтобы его получить необходимо зарегистрироваться на сайте Hugging Face, затем перейдите на [странице создания токенов](https://huggingface.co/settings/tokens/new?globalPermissions=inference.serverless.write&tokenType=fineGrained). Создайте `fine-grained` токен с областью действия `Make calls to the serverless Inference API`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Примечание***. Не распространяйте свой токен в публичных местах, иначе он будет скопрометирован и Hugging Face его удалит! Токен можно скопировать только один раз в моменте его создания. Сохраните его где-нибудь лично у себя и используете только в коде своего приложения когда необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставьте сюда сгенерированный токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_********************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания умного ассистента, с который способен генерировать связанный текст и поддерживать диалог будем использовать модель генерации текста `openai/gpt-oss-20b`. Это недавно выпущенная открытая LLM от OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = \"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим клиента через класс `InferenceClient`, указав ему используемую модель и свой токен доступа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(model=ai_model, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Тестирование работы модели в режиме чата (`chat completion`)\n",
    "Теперь используя объект `client` мы можем выполнять различны задачи. Например, мы можем встпуить в чат с модель, передав ее следюущий промпт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [{\"role\": \"user\", \"content\": \"What is the capital of Great Britan?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это список всех сообщений, то есть история чата. Пока в нем только одно наше сообщение. Формат сообщения такой, что каждое сообщение это словарь, где указываем роль `\"role\"`, а по ключу `\"content\"` находится само сообщение.\n",
    "\n",
    "Распределение ролей:\n",
    "* `\"user\"` - это мы\n",
    "* моодель отвечает под ролью `\"assistant\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передаем через клиента наш запрос. Также указываем максимальный размер ответного соощения `max_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = client.chat_completion(messages=msgs, max_tokens=100, stream=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем, что нам ответила модель ИИ. В ответ мы получаем json формат. Вот так вытаскивем само сообщение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот [здесь](https://huggingface.co/docs/api-inference/tasks/chat-completion) можно почитать подробнее про организацию чата с ИИ и структуру ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей работы необходимо сохранять историю переписки. Поэтому все сообщения (наши и модели) складываем в `msgs` по порядку. Обязательно указываем роли: Роль и контент сообщения от ИИ мы можем вытащить из структуры ответа `ans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs.append({\"role\": ans.choices[0].message.role, \"content\": ans.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот получается такая история переписки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь отправим наше следующее сообщение, чтобы поддержвать разговор. Поинтересуемся, сколько человек проживает в столице. Впишем в наш диалог `msgs` наше сообщения в соответствии с формой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs.append({\"role\": \"user\", \"content\": \"How many peoples lives in the capital city?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжаем разговор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = client.chat_completion(messages=msgs, max_tokens=100, stream=False)\n",
    "print(ans.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И также сохраняем ответ в историю переписки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs.append({\"role\": ans.choices[0].message.role, \"content\": ans.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 ✅\n",
    "\n",
    "Проведите небольшой диалог в таком формате с моделью ИИ. Попробуйте поговорить с ней на русском языке. \n",
    "\n",
    "Попробуйте использовать другую открытую модель, например от Google `google/gemma-3-270m` (https://huggingface.co/google/gemma-3-270m). Для этого при создании клиента укажите новую модель. Вы также можете выбрать любую другую доступную открытую LLM на https://huggingface.co/models, отсортировав модели по задаче \"Text Generation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/ Создание WEB-интерфейса с помощью `Gradio`\n",
    "\n",
    "Создайте приложение Gradio в формате чата. В Gradio есть готовый блок `gr.ChatInterface`. Ему необходимо указать функцию обработки сообщений, в которую вы поместите обращение клиента к модели ИИ как это было выше. Функция принимает на вход два параметра\n",
    "* `message` - текущее сообщение, текст\n",
    "* `history` - история сообщений с чат-ботом, список из словарей, с указанием ролей\n",
    "\n",
    "Например, пусть это будет функция с названием chatbot\n",
    "```python\n",
    "def chatbot(message, history):\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри функции необходимо сформировать историю сообщения с учетом текущего сообщения, то есть добавить в конце `message`. `message` - это просто текст вашего сообщения, его необходимо правильно внести в историю сообщений, то есть указать роль, что сообщение от вашего имени `\"user\"`. Смотрим как это было реализовано выше и делаем также:\n",
    "\n",
    "```python\n",
    "history.append({\"role\": \"user\", \"content\": message})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем добавляем строчку запросу к моделии ИИ\n",
    "```python\n",
    "ans = client.chat_completion(messages=history, max_tokens=1000, stream=False)\n",
    "```\n",
    "\n",
    "Увеличим количество токенов в ответе, чтобы получать развернутые сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце мы должны вернуть текст ответного сообщения, полученного от модели ИИ\n",
    "```python\n",
    "return ans.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге должны получить вот такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(message, history):\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    ans = client.chat_completion(messages=history, max_tokens=1000, stream=False)\n",
    "    return ans.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск интферейса Gradio `gr.ChatInterface`. Указываем функцию, тип сообщений и заголовк блока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(fn=chatbot, type=\"messages\", title=\"Chat with LLM\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЗАДАНИЕ 2 ✅\n",
    "На основе примеров кода выше написать готовое Python приложение в отдельном скрипте. Обратите внимание на параметр `max_tokens`, если вы видите, что ответ модели обрезан, необходимо увеличить значения этого параметра.\n",
    "\n",
    "Дополнительно. При работе чата вы увидите, что ваше введеное сообщение в интерфейсе чата повторяется дважды. Это возникает из-за ошибки формирования истории сообщения в функции `chatbot()`. Вам необходимо исправить эту ошибку, для этого проанализируйте содержимое параметра `history`. Вероятно его необходимо конвертировать в **правильный формат истории сообщения** (см. выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форма отчетности\n",
    "В качестве отчета по лабораторной работе вам необходимо предоставить:\n",
    "1. Файл с кодом вашего приложения\n",
    "2. Скриншот работы графического web-интерфейса вашего приложения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
