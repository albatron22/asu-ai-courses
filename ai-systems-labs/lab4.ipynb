{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Л4: Создание ИИ-ассистента для написания кода\n",
    "\n",
    "В данной работе необходимо создать ИИ-ассистента, который генерирует кода в виде готового файла. У ИИ-ассистента должен быть графический интерфейс (используем Gradio)\n",
    "\n",
    "Для доступа к LLM можно использовать:\n",
    "- **Inference API** для удаленного обращения к LLM (*можно запускать большие модели, но у вас может закончиться лимит*)\n",
    "- Интерфейс `pipeline` из библиотеки `transformers` для локального запуска LLM (*лимит бесконечный, но нельзя запускать большие модели*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Подготовка среды выполнения\n",
    "\n",
    "***Примечание.*** Эти шаги необходимо выполнить, если вы выпорлняете работу локально на компьютере. Если используется онлайн платформу, такую как Google Colab, то пропускаете этот этап.\n",
    "\n",
    "На данной этапе вам наобходимо подготовить виртуальное окружение и установить все необходимые библиотеки.\n",
    "\n",
    "1. Создать и активировать (или только активировать, если ранне создавали) виртуальной окружение `python`.\n",
    "\n",
    "Создайте на диске свою раюочую папку. Откройте терминал и перейдите в свою рабочую директорию\n",
    "```\n",
    "cd path/to/your/workspace\n",
    "```\n",
    "Далее создаем виртуальное окружение с помощью `python-venv`\n",
    "```\n",
    "python -m venv env\n",
    "```\n",
    "*активируем виртуальное окружение*\n",
    "для CMD:\n",
    "```\n",
    "env\\Scripts\\activate\n",
    "```\n",
    "для PowerShell\n",
    "```\n",
    "env\\Scripts\\Activate.ps1\n",
    "```\n",
    "для bash\n",
    "```\n",
    "source env/bin/activate\n",
    "```\n",
    "**Примечание.** `env` - это название вашего виртуального окружения, назвать его можете как угодно.\n",
    "\n",
    "После этого можем выбрать наш локальный интерпрететор pyhton, нажав на кнопку выше \"Select kernel\".\n",
    "\n",
    "2. Устанавливаем все необходимые библиотеки\n",
    "\n",
    "**Примечание.** Библиотеки установятся внутрь вашего виртуального окружения.\n",
    "\n",
    "Нам понадобятся библиотеки Diffusers, Transformers, Accelerate.\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "Также для работы вышеперечисленных бибилиотек потребуется PyTorch:\n",
    "```\n",
    "pip install torch\n",
    "```\n",
    "Библиотека Gradio для создания web-приложения.\n",
    "```\n",
    "pip install gradio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Начало рабты. Создание Inference Client\n",
    "\n",
    "Для бессерверного обращение к модели (то есть без запуска ее на своем каком-то сервере или локально) необходимо отправить запрос, используя Inference API. В результате этого запроса мы получим ответ - Inference (то есть вывод модели). Запрос можно формировать разынми способами, но в этой работе предлагается использовать Python библиотеку `huggingface_hub`. Выполните установку\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта библиотека предоставляет модуль `InferenceClient`, с помощью которого создадим клиент, который будет формировать запрос на сервера Hugging Face. Импортируем этот модуль. Также импортируете Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть одно НО! Для использования Serverless Inference API необходимо иметь **токен доступа**. Чтобы его получить необходимо зарегистрироваться на сайте Hugging Face, затем перейдите на [странице создания токенов](https://huggingface.co/settings/tokens/new?globalPermissions=inference.serverless.write&tokenType=fineGrained). Создайте `fine-grained` токен с областью действия `Make calls to the serverless Inference API`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Примечание***. Не распространяйте свой токен в публичных местах, иначе он будет скопрометирован и Hugging Face его удалит! Токен можно скопировать только один раз в моменте его создания. Сохраните его где-нибудь лично у себя и используете только в коде своего приложения когда необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставьте сюда сгенерированный токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_*******************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания умного ассистента, который умеет писать код лучше нас, будем использовать LLM для генерации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = \"openai/gpt-oss-20b\" # или другая LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим клиента через класс `InferenceClient`, указав ему используемую модель и свой токен доступа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(model=ai_model, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если используете локальный запуск моделей через интерфейс `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ai_model = \"google/gemma-3-1b-it\" # берем модель попроще и полегче\n",
    "model = pipeline(\"text-generation\", model=ai_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Тестирование работы модели в режиме генерации текста (`Text Generation`)\n",
    "Здесь все просто: пишем запрос к ИИ (промт), в котором грамотно, четко и подробно излагаем наши требования. Например, попросим создать приложение, выводящее на графике сумму трех синусоид в реальном времени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write one example python code to output a graph of the sum of three sinusoids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем у нашего клиента `client` метод `text_generation()` с нашим запросом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = client.text_generation(prompt=prompt, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если интерфейс `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохарним вывод модели в файл для дальнейшего прочтения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ans.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(ans) # для pipeline необходимо текст извлечь как ans[0][\"text-generation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откройте файл в текстовом редакторе (можно тут). Вы увидеть текст в разметке **Markdown**. Согласно этой разметке, код помещается между специальными символами \"```\". Скопируйте этот код в блок кода ниже и запустите. Должен сработать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: вставьте сюда код, который придумала модель ИИ\n",
    "# возможно потребуется установить дополнительные библиотеки для работы кода, посмотрите внимательно на импорты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорее всего код у вас заработал! 👩‍💻 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/ ЗАДАНИЕ ✅\n",
    "\n",
    "Требуется создать приложение на основе Gradio, которое с помощью LLM генерирует программный код и выдает его в виде файла с соответствующи расширением.\n",
    "\n",
    "Требования к приложению:\n",
    "\n",
    "- **ВХОДНЫЕ ДАННЫЕ:** текст запроса\n",
    "- **ВЫХОДНЫЕ ДАННЫЕ:** программный код, файл программного кода.\n",
    "  - **ЯП:** по умолчанию просим герерировать Python-код, но вы неограничены в своих предпочтениях\n",
    "- **ГРАФИЧЕСКИЙ ИНТЕРФЕЙС:**\n",
    "  - текстовое поле ввода запроса\n",
    "  - текстовое поле вывода кода (с поддержкой подсветки синтаксиса)\n",
    "  - кнопка выгрузки файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎 *Указанию к выполнению задания*\n",
    "Ответ ИИ помимо кода содержит дополнительный поясняющий текст и т.д., **от которого нужно избавится!**. Это можено решить двумя способами:\n",
    "1. Самостоятельно из всего ответа извлечь только код, котрый находится между специальными символами. Это можно сделать с помощью стандартных средств python работы со строками. Пользуйтесь google, документацией, другим ИИ, чем угодно для самостоятельного решения этой задачи\n",
    "2. [ПРЕДПОЧТИТЕЛЬНО] Так составить промпт к модели, чтобы она выдала вам чистый код. Это называется **промпт-инженерия**, когда вы направляете работу LLM с помощью правильных подсказов (промптов). Например, ваш промпт может начинаться с указанием роли LLM:\n",
    "\n",
    "```python\n",
    "prompt = \"Ты - senior-разработчик python с 20-ти летним стажем...\"\n",
    "```\n",
    "Так LLM пойем кто она и активирует нужные области памяти в неоронной сети. Дале вы можеет писать саму задачу:\n",
    "\n",
    "```python\n",
    "prompt = \"Ты - senior-разработчик python с 20-ти летним стажем. Напиши код на Python, который ... <описываете задачу>\"\n",
    "```\n",
    "\n",
    "В конце следует дать инструкции и описать требования к ответу. Например, если мы хоим получить только чистый код без пояснений, то:\n",
    "```python\n",
    "prompt = \"... В ответе представь исключительно код, не делай пояснения кроме комментариев в самом коде. Код заключи в формат markdown.\"\n",
    "```\n",
    "\n",
    "Полный промпт необязательно прописывать пользователю в интерфейсе. Постоянные части (начало, где указываем роль и конец с требованиями) - это все системные подскази, они не меняются. Поэтому их можно оставить в backend, и формировать полной промпт используюя промпт от пользователя (**где он расписывает задачу** `\"Напиши код, который и т.д.\"`) включая его в системный промпт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форма отчетности\n",
    "В качестве отчета по лабораторной работе вам необходимо предоставить:\n",
    "1. Файл с кодом вашего приложения (Python Script или Jupyter Nonebook)\n",
    "2. Скриншот работы графического web-интерфейса вашего приложения\n",
    "\n",
    "Отчет разместить на **moodle**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
