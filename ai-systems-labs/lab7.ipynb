{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Л7: Использование Inference API для работы с моделями ИИ (Часть 3). Создание ассистента для написания кода\n",
    "\n",
    "**Inference API** - это сервис, который обеспечивает быстрый доступ к многочисленным предобученным моделями ИИ, размещенным на инфраструктуре HuggingFace. Используя его нет необходимости в загрузке модели и запуску модели локально. Таким образом можно создавать прототипы приложений для решения различных задач не переживая о вычислительных мощностях своего компьютера. \n",
    "\n",
    "**Inference API** предоставляет бесплатный и мгновенный доступ к популярным и эффективным моделями для решения следующего спектра задача:\n",
    "* Генерация текста: включает большие языковые модели и подсказки для вызова инструментов, генерируйте и экспериментируйте с высококачественными ответами.\n",
    "* Генерация изображений: легко создавайте персонализированные изображения.\n",
    "* Работа с документами.\n",
    "* Классические задачи ИИ: готовые к использованию модели для классификации текста, изображений, распознавания речи и многого другого.\n",
    "\n",
    "\n",
    "**В этой работе мы познакомимся с этим инструментом и в качестве примера создадим собственного умного ассистента для написания программного кода.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Подготовка среды выполнения\n",
    "На данной этапе вам наобходимо подготовить виртуальное окружение и установить все необходимые библиотеки.\n",
    "\n",
    "1. Создать и активировать (или только активировать, если ранне создавали) виртуальной окружение `python`.\n",
    "\n",
    "В терминале вводим следующие команды команды:\n",
    "\n",
    "*создаем виртуальное окружение с помощью `python-venv`*\n",
    "```\n",
    "python -m venv env\n",
    "```\n",
    "*активируем виртуальное окружение*\n",
    "```\n",
    "env\\Scripts\\activate\n",
    "```\n",
    "**Примечание.** `env` - это название вашего виртуального окружения, назвать его можете как угодно.\n",
    "\n",
    "После этого можем выбрать наш локальный интерпрететор pyhton, нажав на кнопку выше \"Select kernel\".\n",
    "\n",
    "2. Устанавливаем все необходимые библиотеки\n",
    "\n",
    "**Примечание.** Библиотеки установятся внутрь вашего виртуального окружения.\n",
    "\n",
    "Нам понадобятся библиотеки Diffusers, Transformers, Accelerate.\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "Также для работы вышеперечисленных бибилиотек потребуется PyTorch:\n",
    "```\n",
    "pip install torch\n",
    "```\n",
    "Библиотека Gradio для создания web-приложения.\n",
    "```\n",
    "pip install gradio\n",
    "```\n",
    "Также для работы с изображение нам потребуется библиотека PIL (Python Image Library) `Pillow`\n",
    "```\n",
    "pip install Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Начало рабты. Создание Inference Client\n",
    "\n",
    "Для бессерверного обращение к модели (то есть без запуска ее на своем каком-то сервере или локально) необходимо отправить запрос, используя Inference API. В результате этого запроса мы получим ответ - Inference (то есть вывод модели). Запрос можно формировать разынми способами, но в этой работе предлагается использовать Python библиотеку `huggingface_hub`. Выполните установку\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта библиотека предоставляет модуль `InferenceClient`, с помощью которого создадим клиент, который будет формировать запрос на сервера Hugging Face. Импортируем этот модуль. Также импортируете Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть одно НО! Для использования Serverless Inference API необходимо иметь **токен доступа**. Чтобы его получить необходимо зарегистрироваться на сайте Hugging Face, затем перейдите на [странице создания токенов](https://huggingface.co/settings/tokens/new?globalPermissions=inference.serverless.write&tokenType=fineGrained). Создайте `fine-grained` токен с областью действия `Make calls to the serverless Inference API`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Примечание***. Не распространяйте свой токен в публичных местах, иначе он будет скопрометирован и Hugging Face его удалит! Токен можно скопировать только один раз в моменте его создания. Сохраните его где-нибудь лично у себя и используете только в коде своего приложения когда необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставьте сюда сгенерированный токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_*******************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания умного ассистента, который умеет писать код лучше нас, будем использовать модель генерации текста `meta-llama/Meta-Llama-3-8B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим клиента через класс `InferenceClient`, указав ему используемую модель и свой токен доступа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(model=ai_model, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Тестирование работы модели в режиме генерации текста (`Text Generation`)\n",
    "Здесь все просто: пишем запрос к ИИ (промт), в котором грамотно, четко и подробно излагаем наши требования. Например, попросим создать приложение, выводящее на графике сумму трех синусоид в реальном времени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write one example python code to output a graph of the sum of three sinusoids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем у нашего клиента `client` метод `text_generation()` с нашим запросом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = client.text_generation(prompt=prompt, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохарним вывод модели в файл для дальнейшего прочтения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ans.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откройте файл в текстовом редакторе (можно тут). Вы увидеть текст в разметке **Markdown**. Согласно этой разметке, код помещается между специальными символами \"```\". Скопируйте этот код в блок кода ниже и запустите. Должен сработать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: вставьте сюда код, который придумала модель ИИ\n",
    "# возможно потребуется установить дополнительные библиотеки для работы кода, посмотрите внимательно на импорты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорее всего код у вас заработал! 👩‍💻 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/ ЗАДАНИЕ ✅\n",
    "\n",
    "Требуется создать приложение на основе Gradio, которое с помощью моделии языковой модели `meta-llama/Meta-Llama-3-8B-Instruct` генерирует программный код и выдает его в виде файла с соответствующи расширением.\n",
    "\n",
    "Требования к приложению:\n",
    "\n",
    "- **ВХОДНЫЕ ДАННЫЕ:** текст запроса\n",
    "- **ВЫХОДНЫЕ ДАННЫЕ:** программный код, файл программного кода.\n",
    "  - **ЯП:** по умолчанию просим герерировать Python-код, но вы неограничены в своих предпочтениях\n",
    "- **ГРАФИЧЕСКИЙ ИНТЕРФЕЙС:**\n",
    "  - текстовое поле ввода запроса\n",
    "  - текстовое поле вывода кода (с поддержкой подсветки синтаксиса)\n",
    "  - кнопка выгрузки файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎 *Примечание*\n",
    "Ответ ИИ помимо кода содержит дополнительный поясняющий текст и т.д., **от которого нужно избавится!** Здесь вам нужно из всего ответа извлечь только код, котрый находится между специальными символами. Это можно сделать с помощью стандартных средств python работы со строками. Пользуйтесь google, документацией, другим ИИ, чем угодно для самостоятельного решения этой задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форма отчетности\n",
    "В качестве отчета по лабораторной работе вам необходимо предоставить:\n",
    "1. Файл с кодом вашего приложения (Python Script или Jupyter Nonebook)\n",
    "2. Скриншот работы графического web-интерфейса вашего приложения\n",
    "\n",
    "Отчет разместить на [Google диске](https://drive.google.com/drive/folders/1xhGVF935teSij4CIjEqy2XFADXUUBYRc?usp=sharing). \n",
    "\n",
    "Найдите папку вашей подгруппы, в ней создайте папку с вашей фамилией. Именно там вы будете помещать все отчеты по лабораторным занятиям. Для отчета по данной работе создайте папку с именем \"ЛР#\" и поместите туда ваш отчет."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
